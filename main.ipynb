{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries and Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64  # Batch size for training and testing\n",
    "EPOCHS = 10  # Number of training epochs\n",
    "PATCH_SIZE = 4  # Size of each image patch\n",
    "EMBED_DIM = 64  # Dimension of patch embeddings\n",
    "NUM_HEADS = 4  # Number of attention heads\n",
    "NUM_LAYERS = 6  # Number of Transformer encoder layers\n",
    "NUM_CLASSES = 10  # Number of output classes (CIFAR-10)\n",
    "LEARNING_RATE = 1e-3  # Learning rate for optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms: normalize images and convert to tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to mean=0.5, std=0.5\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset (training subset)\n",
    "full_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "subset_size = int(len(full_dataset) * 0.2)  # Use only 20% of the training set\n",
    "subset_indices = torch.randperm(len(full_dataset))[:subset_size]\n",
    "train_data = Subset(full_dataset, subset_indices)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Load CIFAR-10 test set\n",
    "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define the Vision Transformer Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path Embedding Layer\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Converts an image into a sequence of flattened patch embeddings.\"\"\"\n",
    "    def __init__(self, img_size, patch_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)  # Shape: [B, embed_dim, num_patches_height, num_patches_width]\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: [B, num_patches, embed_dim]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer Encoder Block\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Single Transformer encoder block with self-attention and MLP.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)  # Self-attention\n",
    "        x = x + attn_output  # Residual connection\n",
    "        x = self.norm1(x)  # Normalize\n",
    "        x = x + self.mlp(x)  # MLP and residual connection\n",
    "        x = self.norm2(x)  # Normalize\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer model for image classification.\"\"\"\n",
    "    def __init__(self, img_size, patch_size, embed_dim, num_heads, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))  # Class token\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.num_patches, embed_dim))  # Positional encoding\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim, num_heads, mlp_dim=embed_dim * 4) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.head = nn.Linear(embed_dim, num_classes)  # Classification head\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)  # Patch embeddings\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)  # Expand class token for each batch\n",
    "        x = torch.cat((cls_token, x), dim=1)  # Prepend class token\n",
    "        x = x + self.pos_embed  # Add positional encoding\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)  # Pass through Transformer layers\n",
    "        return self.head(x[:, 0])  # Use class token output for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Initialize Model, Optimizer, and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the Vision Transformer model\n",
    "model = VisionTransformer(\n",
    "    img_size=32, patch_size=PATCH_SIZE, embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS, num_layers=NUM_LAYERS, num_classes=NUM_CLASSES\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # Adam optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()  # Set to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, dim=1)  # Get predictions\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total * 100  # Return accuracy percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_accuracies = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate\n",
    "    test_accuracy = evaluate_model(model, test_loader, device)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Plot Training Loss and Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, marker='o', label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Test Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, EPOCHS + 1), test_accuracies, marker='o', color='orange', label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
